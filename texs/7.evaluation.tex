\section{Evaluation}~\label{sec:evaluation}

In this section, we aim to answer the following research questions:


\begin{enumerate}
    \item Does our technique outperform other SAT solvers on certain benchmark families?
    \item Does our technique underperform compared to other SAT solvers on certain benchmark families?
    \item Is our technique less sensitive to encoding choices compared to other \pr learning techniques?
\end{enumerate}

We do so by implementing our technique in a tool \tool (a fork of \cadical commit f13d74439a5b5c963ac5b02d05ce93a8098018b8). In \autoref{subsec:pigeonhole-results}, we compare \tool to standard SAT solvers as well as \pr learning techniques. We evaluate based on time taken, the length of the proof, and the sensitivity to renaming variables and reordering clauses.

Second, in \autoref{subsec:satcomp-results}, we compare \tool to other SAT solvers on the benchmarks from the annual Satisfiability competition from the years 2022, 2023, and 2024. 

Third, we analyze the use of specific heuristic choices in \tool by turning heuristics off one by one and observing the effect on the performance of \tool. This is described in \autoref{subsec:analysis-of-heuristics}.

Finally, we conclude, in \autoref{subsec:discussion}, by discussing the benchmarks families upon which our technique performs well and poorly.


\subsection{Pigeonhole results}~\label{subsec:pigeonhole-results}

\subsection{SATCOMP results}~\label{subsec:satcomp-results}

\subsection{Analysis of heuristics}~\label{subsec:analysis-of-heuristics}

\subsection{Discussion of Benchmark Families}~\label{subsec:discussion}