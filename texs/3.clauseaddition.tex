\section{Methodology}~\label{sec:method}

Our methodology is based on four steps:
\begin{algorithm}
    \caption{Our Methodology}\label{alg:methodology}
    \SetAlgoNoLine
    \SetKwFunction{Method}{Method}
    \SetKwFunction{LCP}{LCP}
    \SetKwFor{For}{for}{:}{}
    \SetKwFor{If}{if}{:}{}
    \SetKwProg{Fn}{Function}{:}{}
    \SetKwBlock{Begin}{}{}

    \Fn{LearnClause($\psi$, $\alpha$)}{
        \For{$i \in vars(\psi)$}{
            \For{$j \in vars(\psi)$}{
                propagate($i$)
                propagate($j$)
                $\alpha_c, \alpha_a := \LCP(\alpha)$
                clause := shrink($\alpha_c, \alpha_a$)
                $\psi := \psi \land clause$
            }
        }
    }
\end{algorithm}

\subsection{Learning Clauses}~\label{subsec:learning}

As described in \autoref{subsec:autarkies}, we can split a partial assignment $\alpha = \alpha_c \sqcup \alpha_a$ and use this to learn a \pr clause. Since all of $\alpha_c$ appears in such a clause, we must try to keep in as small as possible.

We present this algorihtm from Kiesel et al \cite{conditionalautarkies} to find the smallest possible $\alpha_c$:


\begin{algorithm}
    \caption{Minimiazing $\alpha_c$ in $\alpha = \alpha_c \sqcup \alpha_a$}\label{alg:leastcond}
    \SetAlgoNoLine
    \SetKwFunction{LCP}{LeastConditional}
    \SetKwFor{For}{for}{:}{}
    \SetKwFor{If}{if}{:}{}
    \SetKwProg{Fn}{Function}{:}{}
    \SetKwBlock{Begin}{}{}

    \Fn{\LCP{$\psi$, $\alpha$}}{

        $\alpha_c := \emptyset$\;
        \For{$C \in \phi$}{
            \If{$\alpha$ touches $C$ without satisfying $C$}{
                $\alpha_c := \alpha_c \cup (\alpha \cap \overline{C})$\;
            }
        }
        \Return{$\alpha_c$}\;
    }
\end{algorithm}

We can then compute $\alpha_a := \alpha \backslash \alpha_c$. It is pretty easy to see why this is a conditional autarky, since every clause that $\alpha_a$ touches, must be satisfied by some literal in $\alpha$.

We can see that $\alpha_c$ is minimal since for each clause that is touched, we add in those literals from the assignment that touch and do not satisfy the clause. They must be in $\alpha_c$, since otherwise $\alpha_a$ will touch a clause that is not satisfied in $\alpha_a$ and $\alpha_c$, violating the conditional autarky property.

\subsection{Shrinking Clauses}~\label{subsec:shrinking}

While we minimize $\alpha_c$, this still leads to a long clause in many cases. However, we can shrink the clause using some clever techniques. Notice that if $\alpha_c = c_1, ..., c_n$ and $\alpha_a = a_1, ..., a_m$, we could add clauses of the form $\overline{c_1} \lor ... \lor \overline{c_n} \lor a_i$ where $1 \leq i \leq m$.


% Consider $\overline{\alpha_a}$, the negation of the literals in $\alpha_a$. 
We consider the set $C_0 = \{c_j \in C| \exists a_i \in \alpha_a \; \overline{a_i} \impunit^\phi c_j \}$. This is essentially the set of literals in $\alpha_c$ that can be implied by unit propagation by negating some literal in $\alpha_a$.
%  in the code we propagate -alpha_a[i]
%  then if val (neg_alpha_c[j]) < 0, we add neg_alpha_c[j] to propagated and
%  alpha_a[i] to useful

We also define $A_0 \subseteq \alpha_a$ as a set such that for each $c \in C_0$, there is some $a \in A_0$ such that $\overline{a} \impunit^\phi c$. By definition, there must be some $A_0$, but there could be many possible $A_0$. We will discuss how we pick $A_0$ in \autoref{subsec:sym} as it will be very important for having a technique resistant to different encodings. 


% Instead we look at $\overline{I} = \{x_{2, 1}, x_{1, 2}\}$. Then take the set $\overline{I} \vdash_1 C_0 \subseteq C$, i.e., the maximal subset of $C$ that $\overline{I}$ implies via unit propagation.

We want learn the clause $\bigvee_{c \in C \backslash C_0} \overline{c} \lor \bigvee_{a \in A_0} a$


\begin{theorem}~\label{thm:shrunkgbcequisat}
    The formula $\phi$ is satisfiable if and only if $\phi \land (\bigvee_{c \in C \backslash C_0} \overline{c} \lor \bigvee_{a \in A_0} a)$ is satisfiable.
\end{theorem}

\begin{proof}
    \underline{$\Leftarrow$:} This is immediate


    \underline{$\Rightarrow$:}  As a corollary to \autoref{thm:gbcequisat}, $\phi$ is satisfiable if and only if $\phi \land (\bigvee_{c \in C} \overline{c} \lor \bigvee_{a \in A_0} a)$ is satisfiable.

    Thus we can assume $\phi \land (\bigvee_{c \in C} \overline{c} \lor \bigvee_{a \in A_0} a)$ is satisfiable by some satisfying assignment $\beta$.

    We claim $\beta$ is a satisfying assignment for $\phi \land (\bigvee_{c \in C \backslash C_0} \overline{c} \lor \bigvee_{a \in A_0} a)$. This could only not be the case if (1)~for all $a \in A_0$ $\overline{a} \in \beta$ and (2)~there is some $c \in C_0$ such that $\overline{c} \in \beta$. 

    But by definition there is some $a \in A_0$ such that $\overline{a} \impunit^\phi c$. Thus $\phi \land \overline{a} \land \overline{c}$ is unsatisfiable via unit propagation. However, this cannot be the case since $\overline{a}, \overline{c} \in \beta$ and $\beta$ is a satisfying assignment for $\phi$.
\end{proof}

\subsection{Resilience to Encoding}~\label{subsec:sym}

As mentioned in \autoref{subsec:shrinking}, our choice for $A_0$ can matter a lot for which clause we learn. 


\noindent \textbf{Greedy Set Cover:} %~\label{subsubsec:greedysetcover}
In order to minimize the size of the clause, we may want the smallest possible $A_0$ that maximizes $C_0$. Say for each $a \in \alpha_a$, we define $\alpha_a^{SETS}(a) = \{ c \in \alpha_c \; | \; \overline{a} \impunit^\phi c\}$. 

In the case that $C_0 = \alpha_C$, finding the smallest $A_0$ is exactly the set cover problem with $\alpha_a^{SETS}$. This is NP-hard and we do not want to spend time figuring this out. Instead, we can use the greedy algorithm to solve, which is a $1 + \ln |\alpha_c|$ approximation algorithm. Specifically, the greedy algorithm will find the largest set in $\{\alpha_a^{SETS} \cap (\alpha_c \backslash C_0) | a \in \alpha_a \}$ at each step and add it to $C_0$. It stops when each of the $\alpha_a^{SETS} \cap (\alpha_c \backslash C_0)$ are empty.
% either $C_0 = \alpha_C$ or there are no terms 


\begin{algorithm}
    \caption{Algorithm finding $A_0$}\label{alg:finda0}
    \SetAlgoNoLine
    \SetKwFunction{FindA}{FindA0}
    \SetKwFor{For}{for}{:}{}
    \SetKwFor{If}{if}{:}{}
    \SetKwProg{Fn}{Function}{:}{}
    \SetKwBlock{Begin}{}{}

    \Fn{\FindA{$\psi$, $\alpha_a$, $\alpha_c$}}{
        $\alpha_a$ := \texttt{sort($\alpha_a$)}\;
        $\alpha_a^{SETS}$ := init \texttt{Array[len($\alpha_a$)]}\;
        \For{$i \in \texttt{range}(\alpha_a)$}{
            \texttt{propagate($-\alpha_a[i]$)}\;
            \texttt{implied} := $\{\}$\;
            \For{$c \in \alpha_c$}{
                \texttt{propagate(-c)}\;
                \If{\texttt{unsat}}{
                $\texttt{implied} := \texttt{implied} \cup \{c\}$\;
            }
            }
            $\alpha_a^{SETS}[i]$ := \texttt{implied}\;
        }
        \Return{\texttt{greedySetCover}($\alpha_a^{SETS}$)}\;
    }
\end{algorithm}
% todo : this algorithm is not actually what we do, since we don't propagate on c -> see if this is a problem ever

In \autoref{alg:finda0}, we describe our process for calculating $A_0$. We initially pre-sort $alpha_a$ by implication, which we will later discuss in more detail. Then we intialize $\alpha_a^{SETS}$ as an array, we iterate the counter $i$ through $\alpha_a$, populating $\alpha_a^{SETS}[i]$ with the set of literals in $\alpha_c$ that are implied by $-\alpha_a^{SETS}[i]$. Finally, we apply \texttt{greedySetCover}



\noindent \textbf{Sorting $\alpha_a$ by Implication:}%~\label{subsubsec:impordering}
The greedy set cover algorithm described above is almost sufficient for being insensitive to heuristics, but there is one other optimization that we made. \autoref{alg:finda0} describes sorting $\alpha_a$ as an initial step. This is important for cases where there are more than one cover of the same size. The most common occurence of this is for $a_1, a_2 \in \alpha_a$, we may have that $\overline{a_2} \impunit^\phi \overline{a_1} \impunit^\phi c_1, ..., c_n$. 

Thus, greedy set cover could pick $\{a_1\}$ or $\{a_2\}$ as singleton sets. However, since $a_1 \impunit^\phi a_2$, we really want to learn the unit clause $a_1$ since it is much more powerful.

\subsection{Other Heuristics}~\label{subsec:heuristics}

We employ several other heuristics:

\noindent \textbf{Checking if trivial:} Introducing globally blocked clauses can affect how we divide $\alpha = \alpha_c \sqcup \alpha_a$ as done in \autoref{alg:leastcond}. Thus, it is better to avoid introducing globally blocked clauses if they are not useful. We do this by checking if $\phi \impunit C$ for each potential clause $C$. If it is implied by unit propagation, we do not learn it since we consider it easy to learn.

