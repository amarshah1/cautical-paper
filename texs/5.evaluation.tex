\section{Evaluation}~\label{sec:evaluation}

\begin{figure*}[!t]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/pigeonhole_runtime_comparison.jpg}
        \label{fig:pigeonhole-runtime-comparison}
    \end{subfigure}
    \hspace{0.06\textwidth}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/pigeonhole_proof_size_comparison.jpg}
        \label{fig:pigeonhole-proof-size-comparison}
    \end{subfigure}
    \caption{Comparison of \tool, \cadical, \sadical, and \prelearn on pigeonhole principle benchmarks up to size $40$. The y-axis is on a cube root scale. The performance of a solver on the original benchmark is shown with a solid line. The median of 5 scranfilized queries is shown with a dashed line. If a solver times out on a query in 5000s, it is not shown.}
    \label{fig:pigeonhole-results}
\end{figure*}

In this section, we empirically evaluate our technique against other PR clause
learning techniques. In doing so, we aim to answer the following research
questions:


\begin{enumerate}[label={RQ\arabic*}]
    \item Can our approach provide a speedup on certain benchmark families?
    \item Is our approach less sensitive to encoding choices compared to other
    PR learning techniques?
\end{enumerate}


We compare the two main tools learning PR clauses: \sadical (based on SDCL) and
\prelearn (a preprocessing technique that calls \sadical). To be consistent with
our approach, we run \prelearn with its default settings for 30 seconds, then
solve the preprocessed formula with \cadical. We also compare to \cadical as a
baseline with no PR clause learning. We check all proofs of unsatisfiability
from \tool using \texttt{dpr-trim}~\cite{dpr-trim}.

All experiments were performed in the Anvil Supercomputing Center on nodes with
128 cores and 2 GB RAM per core~\cite{anvil}. We ran 64 experiments in parallel
per node with a 5,000 second timeout, the default timeout for the SAT
competition.

In \autoref{subsec:eval-pigeonhole}, we compare all approaches on the pigeonhole
principle, evaluating runtime, proof length, and sensitivity to the encoding of
the formula. In \autoref{subsec:eval-satcomp}, we evaluate the solvers on
benchmarks from the '22, '23, and '24 SAT competition's main
tracks~\cite{satcomp2022,satcomp2023,satcomp2024}. In
\autoref{subsec:eval-discussion}, we highlight certain benchmark families that
benefit from PR clause learning. In \autoref{sec:heuristics}, we evaluate the 
benefit of different heuristic choices in \tool. Finally, we conclude in 
\autoref{subsec:researchquestions} with a discussion of how performed on RQ1 and 
RQ2.


\subsection{Pigeonhole results}~\label{subsec:eval-pigeonhole}


Approaches based on SDCL, such as \sadical, are successful for learning $O(n^3)$
proofs for the pigeonhole principle, but are very sensitive to the encoding of
the formula. We compare the solvers on pigeonhole principle from \ph{2} to
\ph{40} and plot these results in \autoref{fig:pigeonhole-results}. As the
expected best-behavior is cubic, we use a cube root scale for the y-axis.

As expected, \cadical grows exponentially, while \sadical and \tool scale
cubicly in both runtime and proof size. Significantly, \tool is able to learn
$3.59$-$3.64\times$ shorter proofs compared to \sadical. 
\prelearn scales cubicly on small formulas, but for \ph{22} and larger, will not
learn enough useful PR clauses in the preprocessing step and will timeout after
spending the rest of its time running \cadical.

Additionally, we evaluate all solvers on scranfilized variations of the
pigeonhole principle. Scranfilization is a technique for generating an
satisfiability-equivalent formula~\cite{scranfilize}. We use the tool
\texttt{scranfilize}~\cite{scranfilize} with the options permuting variables,
permuting clauses, and flipping literals (with probability $0.5$) all turned on.
We run each solver on 5 scranfilized variations for each benchmark and take the
median runtime and proof size. This is shown in \autoref{fig:pigeonhole-results}
with dashed lines.

\sadical and \prelearn exhibit an exponential trend for runtime and proof size
on the scranfilized benchmarks. \sadical will spend all its time in the main
SDCL loop not learning enough useful clauses. \prelearn will learn some useful
PR clauses in preprocessing, but not enough to sufficiently shrink the search
space for formulas larger than \ph{16}.

On the other hand, \tool almost matches its non-scranfilized performance,
demonstrating that it learns useful PR clauses regardless of the encoding.

\subsection{SAT competition results}~\label{subsec:eval-satcomp}








\begin{figure*}[!t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/cadical_vs_cautical_nontrivial.jpg}
        \caption{Comparing \tool to \cadical.}
        \label{subfig:cautical-vs-cadical-satcomp}
    \end{subfigure}
    \hspace{0.06\textwidth}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/prelearn_vs_cautical_nontrivial.jpg}
        \subcaption{Comparing \tool to \prelearn.}
        \label{subfig:cautical-vs-prelearn-performance}
    \end{subfigure}
    \caption{Performance comparison of \tool with \prelearn and \cadical on SAT competition benchmarks. On each graph, we filter out all benchmarks where neither solver learns any PR clauses. The color indicates the number of PR clauses learnt by \tool.}
    \label{fig:solver-comparison}
\end{figure*}

\begin{table}[h]
    \centering
    \captionsetup{position=above}
    \caption{Number of formulas that \cadical, \prelearn and \tool solve, divided between 0-10k clause and 10k-20M clause formulas, and SAT and UNSAT formulas. For \prelearn and \tool we include the number of benchmarks with PR clauses learnt, more than $50$ PR clauses learnt, improved on \cadical by 5\% or more; and solved that \cadical did not solve.}
    \sisetup{table-format=3}    
    \begin{tabular}{lrrrrr}
      \toprule
      & \multicolumn{2}{c}{0--10k clause} & \multicolumn{2}{c}{10k--20M clause}
      \\
      \cmidrule(lr){2-3} \cmidrule(lr){4-5} & SAT & UNSAT & SAT & UNSAT & Total
      \\
      \midrule
    %   Total Formulas & 70 & 132 & 395 & 416 & 1099 \\
      \cadical Solved  &  54 &  73 & 319 & 303 & 749 \\
      \midrule
      \prelearn \\
      \; Total &  52 &  90 & 322 & 307 & 771 \\
      \; Learn PR clause   &  40 &  73 & 179 & 145 & 431\\
      \; Learn $>50$ PR clauses   &  22 &  51 & 104 &  79 & 256\\
      \; Improve on \cadical &  11 &  42 &  57 &  36 & 146\\
      \; Unique from \cadical &   1 &  17 &   6 &   6 & 30 \\
      \midrule
      \tool \\
      \; Total &  52 &  87 & 317 & 298 & 754 \\
      \; Learn PR clause     &   16 &  58 &  30 &  35 & 139 \\
      \; Learn $>$$50$ PR clauses  &   1  &  39 &  6 &  11 & 57 \\
      \; Improve on \cadical &  23  &  48 &  89 &  59 & 219 \\
      \; Unique from \cadical &   0 &  18 &   9 &   9 & 36 \\
      \bottomrule
    \end{tabular}
    \label{tab:solver-stats}
  \end{table}

\begin{figure*}[!ht]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/cdf.png}
        \caption{The number of formulas solved by \tool, \prelearn, and \cadical.}
        \label{fig:cdf}
    \end{minipage}
    \hspace{0.06\textwidth}
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/clauses_histogram.jpg}
        \caption{The size of PR clauses learnt by \tool and \prelearn}
        \label{fig:clauses-histogram}
    \end{minipage}
\end{figure*}

\begin{figure*}[!h]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figs/cadical_vs_cautical_interesting.jpg}
            \label{fig:cautical-vs-cadical}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/prelearn_vs_cautical_interesting_legend.jpg}
        \label{fig:cautical-vs-prelearn}
    \end{subfigure}

    \caption{Comparing \tool with \cadical and \prelearn on various benchmark families.}
    \label{fig:solver-comparison-familis}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \input{figs/heuristics.tex}
    \caption{Performance comparison of \tool with various heuristics turned on and off}~\label{fig:global-heuristics}
\end{figure*}

We compare the performance of \tool with \cadical and \prelearn on the
benchmarks from the '22, '23, and '24 SAT competition's main
tracks~\cite{satcomp2022,satcomp2023,satcomp2024}. We remove duplicates and
exclude all benchmarks with more than twenty million clauses as these are out of
scope for our technique. This gives us a total of 1,089 benchmarks. We exclude
\sadical from our evaluation as it only solves 22 of these benchmarks.

\autoref{tab:solver-stats} shows the number of instances solved by each solver.
Out of the total number solved, it shows the number of formulas for which
\prelearn and \tool learn additional PR clauses, improve upon \cadical by at
least 5\%, and solve a formula that \cadical does not solve. 

We divide the benchmarks based on number of clauses (0-10k or 10k-20M) and
status (SAT or UNSAT). The number of clauses is a good indicator of the type of
benchmark, with many hard combinatorial problems containing fewer than ten
thousand clauses.

\autoref{fig:solver-comparison} shows \tool and \cadical's performance relative
to \cadical.
\autoref{subfig:cautical-vs-cadical-satcomp} shows that \tool learns a large
number of PR clauses for formulas which it solves quickly and \cadical times
out on. \autoref{subfig:cautical-vs-prelearn-performance} shows that \prelearn
also solves a number of formulas that \tool cannot, but \tool typically performs
better on formulas where it learns many PR clauses.

We find that \tool can show a performance improvement or degradation on formulas
where it does not learn any PR clauses. This is because as \tool runs, it
updates internal data structures such as watched literals, clause occurrence
lists, and variable phases. 

For instance, during the initial phase of searching for PR clauses, the solver
will decide on a literal. Unit propagation of this literal may lead to a
conflict, and the solver can learn the unit clause without any autarky/PR
reasoning. Unit clauses are not stored or treated as learned clauses inside
\cadical, but instead the literal becomes “fixed,” i.e., the solver treats the
literal as always true. For instance, this happens on the three satcoin
benchmarks where \tool shows the most improvement (see
\autoref{fig:solver-comparison-familis}).


PAR-2 score is a standard metric used to evaluate the performance of solvers. It
is evaluated as the sum of the runtimes of solved instances and twice the
timeout of unsolved instances. On this dataset, \cadical has a PAR-2 score of
3522 seconds, \prelearn has a PAR-2 score of 3331 seconds, and \tool has a PAR-2
score of 3442 seconds. 

\autoref{fig:cdf} shows the distribution of benchmarks solved by each solver.
Both \prelearn and \tool lag behind \cadical during the 30 second preprocessing
stage, but eventually solve more benchmarks. In the end, \prelearn solves the most
formulas at 771, followed by \tool at 754, and \cadical at 749. 


\tool exercises more selective PR clause learning techniques, only learning PR
clauses for 139 formulas, while \prelearn learns PR clauses for 431 formulas
(see \autoref{fig:clauses-histogram}). On those formulas, \tool improves
\cadical's runtime by 5\% or more on 29.9\% of benchmarks, while \prelearn improves it on
24.7\% of benchmarks.


\subsection{Discussion of Benchmark Families}~\label{subsec:eval-discussion}

We identify six benchmark families for which PR clauses perform well. We choose
them based on prior work~\cite{prelearn} and our experiments on SAT competition
benchmarks:

\begin{enumerate}
    \item \texttt{mutilated-chessboard}: famous problem asking if one can use
    $2$-by-$1$ tiles to cover an $2n$-by-$2n$ chessboard with opposite corners
    removed. This is difficult for
    resolution~\cite{mutilatedchessboard-exponential}, but there exists $O(n^3)$
    \pr proofs~\cite{mutilatedchessboard-pr}.
    \item \texttt{perfect\_matching}: generalization of the pigeonhole principle
    and mutilated chessboard problems with various at-most-one
    constraints~\cite{bipartgen}.
    \item \texttt{register\_allocation}: the graph coloring problem generated by
    simulating register allocation on individual Python
    functions~\cite{register-allocation}.
    \item \texttt{relativized\_pigeonhole}: generalization of the pigeonhole
    principle where we place $n+1$ pigeons in $n$ holes with $k$ nesting places.
    \item \texttt{satcoin}: variant of a bitcoin mining problem~\cite{satcoin}.
    \item \texttt{test\_configuration}: is there a list of configurations of
    size $k$ that covers every pairwise combination of configurations of a SAT
    solver~\cite{test-configuration}.
\end{enumerate}

\begin{table}[h]
    \centering
    \captionsetup{position=above}
    \caption{Overview of benchmark families used in evaluation}
    \begin{tabular}{lll}
        \hline
        Benchmark Family & Number of Clauses  & \tool \# PR  \\
        \hline
        mutilated-chessboard & 900-3K & 115-351 \\
        perfect\_matching & 300-1K & 183-447 \\
        register\_allocation & 1K-23K & 671-3322 \\
        relativized\_pigeonhole & 3K-2M & 469-9091 \\
        satcoin & 600K-600K & 0-0 \\
        test\_configuration & 31K-64K & 0-0 \\
        \hline
    \end{tabular}
    
    \label{tab:benchmark-families}
\end{table}

\autoref{fig:solver-comparison-familis} compares the performance of \tool with
\cadical and \prelearn on these benchmark families. We do not evaluate \sadical
as it does very poorly on SAT competition benchmarks.
\autoref{tab:benchmark-families} provides a brief overview of the families: the
number of clauses in the original formulas (up to 1 significant digit) and the
number of PR clauses learnt by \tool.

\tool compares favorably to \cadical on all benchmark families, showing especially large speedups on
\texttt{register\_allocation}, \texttt{mutilated\_chessboard} and
\texttt{perfect\_matching}. 

\prelearn exhibits large speedups over \tool on \texttt{test\_configuration} and
\texttt{perfect\_matching}. \tool performs better on smaller instances of
\texttt{register\_allocation} and \texttt{mutilated\_chessboard}, while
\prelearn performs better on larger instances. \tool can also solve instances of
\texttt{satcoin} that \prelearn cannot.

\subsection{Heuristics}~\label{sec:heuristics}

\autoref{fig:global-heuristics} shows the performance of \tool with different
heuristics (discussed in \autoref{sec:implementation}) turned on and off. We
evaluate on the different benchmark families discussed in
\autoref{subsec:eval-discussion}.

First we consider turning three optimizations off.
\autoref{fig:global-no-shrink} compares \tool to \tool with
\textsf{shrink} turned off, i.e. we do not learn PR clauses.
\autoref{fig:globaldontfilter} compares \tool to \tool with
\textsf{filter-triv} off, i.e. we do not filter trivial clauses.
\autoref{fig:global-max-length} compares \tool to \tool with
\textsf{filter-long} set to $10$, i.e. we filter out clauses of size $> 10$ (as
opposed to $2$ by default).

When we disable any of three optimizations, the performance is significantly
worse, especially on \texttt{perfect-matching}, \texttt{mutilated-chessboard},
and \texttt{register-allocation} benchmarks.

Next, we consider turning three potential ``optimizations'' on.
\autoref{fig:global-time-limit} sets \textsf{longer-preprocess} to 100 seconds
(as opposed to 30 seconds by default). \autoref{fig:global-sort-i} turns on
\textsf{order-i}, propagating the first literal $i$ ordered by which literals
occurs most frequently in the original formula. Finally,
\autoref{fig:global-heuristics} turns on \textsf{select-j}, picking $j$ the
second literal by whether it ``touches'' the first literal $i$.

Enabling any of these three ``optimizations'' does not improve performance and
in the case of \textsf{order-i}, it slightly hurts performance.

\subsection{Research Questions}~\label{subsec:researchquestions}

To conclude, we discuss the two research questions. 

For RQ1, \tool decisively outperforms all solvers except \sadical on the
pigeonhole benchmarks. On SAT competition benchmarks, \tool improves \cadical's
PAR-2 score by 2.2\%. \prelearn performs even better, improving \cadical's PAR-2
score by 5.4\%. However, on formulas where \tool learns a \pr clause, it improves
\cadical's runtime by 5\% or more on 29.9\% of benchmarks. On formulas where \prelearn 
learns a \pr clause, it improves \cadical's runtime by 5\% or more on 24.7\% of benchmarks.
 
For RQ2, \tool is the only solver to solve all pigeonhole formulas after
scranfilization. Indeed, scranfilization has a negligible effect on \tool's
performance.

We experienced some success with robustness on the SAT competition benchmarks,
but not as significant as for pigeonhole. For instance, in the initial stage
when propagating first on literal $i$ (see \autoref{alg:methodology}), we
randomly pick the order for literal $i$.

Prior PR learning approaches such as \prelearn used specific orderings of
literals to their advantage. We evaluated such an ordering in
\autoref{fig:global-sort-i} and found that it did not make a difference for
\tool.

